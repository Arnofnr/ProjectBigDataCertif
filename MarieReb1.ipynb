{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lire les données et les afficher (repris du code de Olivier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données d'entrainement complètes :\n",
      "     ID  Day  Number_of_Customers  Open  Promo  State_Holiday  School_Holiday  \\\n",
      "0  625    3                  641     1      1              1               0   \n",
      "1  293    2                  877     1      1              1               1   \n",
      "2   39    4                  561     1      1              1               0   \n",
      "3  676    4                 1584     1      1              1               0   \n",
      "4  709    3                 1477     1      1              1               0   \n",
      "\n",
      "   Month  Day_of_month  Year  Sales  \n",
      "0     11             6  2013   7293  \n",
      "1      7            16  2013   7060  \n",
      "2      1            23  2014   4565  \n",
      "3      9            26  2013   6380  \n",
      "4      1            22  2014  11647   \n",
      "\n",
      "Données de test :\n",
      "     ID  Day  Number_of_Customers  Open  Promo  State_Holiday  School_Holiday  \\\n",
      "0  249    5                  725     1      1              1               0   \n",
      "1  190    4                  564     1      0              1               0   \n",
      "2  850    3                  644     1      1              1               0   \n",
      "3  776    6                  435     1      0              1               0   \n",
      "4   14    4                  799     1      1              1               0   \n",
      "\n",
      "   Month  Day_of_month  Year  \n",
      "0     12            19  2014  \n",
      "1      2            28  2013  \n",
      "2      6             5  2013  \n",
      "3      3            28  2015  \n",
      "4     10             2  2014   \n",
      "\n",
      "Types des données d'entrainement complètes : \n",
      " ID                     int64\n",
      "Day                    int64\n",
      "Number_of_Customers    int64\n",
      "Open                   int64\n",
      "Promo                  int64\n",
      "State_Holiday          int32\n",
      "School_Holiday         int64\n",
      "Month                  int32\n",
      "Day_of_month           int32\n",
      "Year                   int32\n",
      "Sales                  int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "path='/Users/marierebiere/Desktop/cours/supaero/4A/Bigdata2/public_dat/'  # si les données sont déjà dans le répertoire courant\n",
    "\n",
    "input_string_train = path + 'store_train.data'\n",
    "\n",
    "input_string_train = \"store_train.data\"\n",
    "#df_train = pd.DataFrame([x.split(' ') for x in input_string_train.split('\\n')],columns=[\"ID\",\"Day\",\"Date\",\"Number_of_Customers\",\"Open\",\"Promo\",\"State_Holiday\",\"School_Holiday\"],\n",
    "                  #dtype=int)\n",
    "                  #{\"ID\":int,\"Day\":int,\"Date\":object,\"Number_of_Customers\":int,\"Open\":int,\"Promo\":int,\"State_Holiday\":int,\"School_Holiday\":int})\n",
    "df_train = pd.read_csv(input_string_train ,sep=\" \",header=None, names = [\"ID\", \"Day\", \"Date\", \"Number_of_Customers\", \"Open\", \"Promo\", \"State_Holiday\", \"School_Holiday\"])\n",
    "\n",
    "df_train[\"Date\"]=pd.DataFrame(df_train[\"Date\"],dtype=\"datetime64[ns]\")\n",
    "df_train[\"State_Holiday\"]=pd.DataFrame(df_train[\"State_Holiday\"],dtype=\"bool\")\n",
    "df_train[\"State_Holiday\"]=pd.DataFrame(df_train[\"State_Holiday\"],dtype=\"int\")\n",
    "\n",
    "df_train['Month'] = df_train['Date'].map(lambda d: d.month)\n",
    "df_train[\"Month\"]=pd.DataFrame(df_train[\"Month\"],dtype=int)\n",
    "df_train['Day_of_month'] = df_train['Date'].map(lambda d: d.day)\n",
    "df_train[\"Day_of_month\"]=pd.DataFrame(df_train[\"Day_of_month\"],dtype=int)\n",
    "df_train['Year'] = df_train['Date'].map(lambda d: d.year)\n",
    "df_train[\"Year\"]=pd.DataFrame(df_train[\"Year\"],dtype=int)\n",
    "\n",
    "del df_train[\"Date\"]\n",
    "\n",
    "# Lire les données de test\n",
    "input_string_test = path + 'store_test.data'\n",
    "\n",
    "input_string_test = \"store_test.data\"\n",
    "df_test = pd.read_csv(input_string_test ,sep=\" \",header=None, names = [\"ID\", \"Day\", \"Date\", \"Number_of_Customers\", \"Open\", \"Promo\", \"State_Holiday\", \"School_Holiday\"])\n",
    "df_test[\"Date\"]=pd.DataFrame(df_test[\"Date\"],dtype=\"datetime64[ns]\")\n",
    "df_test[\"State_Holiday\"]=pd.DataFrame(df_test[\"State_Holiday\"],dtype=\"bool\")\n",
    "df_test[\"State_Holiday\"]=pd.DataFrame(df_test[\"State_Holiday\"],dtype=\"int\")\n",
    "\n",
    "df_test['Month'] = df_test['Date'].map(lambda d: d.month)\n",
    "df_test[\"Month\"]=pd.DataFrame(df_test[\"Month\"],dtype=int)\n",
    "df_test['Day_of_month'] = df_test['Date'].map(lambda d: d.day)\n",
    "df_test[\"Day_of_month\"]=pd.DataFrame(df_test[\"Day_of_month\"],dtype=int)\n",
    "df_test['Year'] = df_test['Date'].map(lambda d: d.year)\n",
    "df_test[\"Year\"]=pd.DataFrame(df_test[\"Year\"],dtype=int)\n",
    "\n",
    "del df_test[\"Date\"]\n",
    "\n",
    "##### solutions\n",
    "input_string_sol = path + 'store_train.solution'\n",
    "input_string_sol = \"store_train.solution\"\n",
    "df_sol = pd.read_csv(input_string_sol ,sep=\" \",header=None, names = [\"Sales\"])\n",
    "\n",
    "\n",
    "df_full_train=pd.concat([df_train,df_sol],axis=1)\n",
    "\n",
    "print(\"Données d'entrainement complètes :\\n\", df_full_train.head(), \"\\n\")\n",
    "print(\"Données de test :\\n\", df_test.head(), \"\\n\")\n",
    "print(\"Types des données d'entrainement complètes : \\n\", df_full_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vérifie que le tableau est bien rempli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb de jours 712045\n",
      "Nb de magasins 712045\n",
      "Nb de Customers 712045\n",
      "Nb de open 712045\n",
      "Nb de Promo 712045\n",
      "Nb de State_Holiday 712045\n",
      "Nb de School_Holiday 712045\n",
      "Nb de Sales 712045\n",
      "           ID    Day  Number_of_Customers   Open  Promo  State_Holiday  \\\n",
      "0       False  False                False  False  False          False   \n",
      "1       False  False                False  False  False          False   \n",
      "2       False  False                False  False  False          False   \n",
      "3       False  False                False  False  False          False   \n",
      "4       False  False                False  False  False          False   \n",
      "...       ...    ...                  ...    ...    ...            ...   \n",
      "712040  False  False                False  False  False          False   \n",
      "712041  False  False                False  False  False          False   \n",
      "712042  False  False                False  False  False          False   \n",
      "712043  False  False                False  False  False          False   \n",
      "712044  False  False                False  False  False          False   \n",
      "\n",
      "        School_Holiday  Month  Day_of_month   Year  Sales  \n",
      "0                False  False         False  False  False  \n",
      "1                False  False         False  False  False  \n",
      "2                False  False         False  False  False  \n",
      "3                False  False         False  False  False  \n",
      "4                False  False         False  False  False  \n",
      "...                ...    ...           ...    ...    ...  \n",
      "712040           False  False         False  False  False  \n",
      "712041           False  False         False  False  False  \n",
      "712042           False  False         False  False  False  \n",
      "712043           False  False         False  False  False  \n",
      "712044           False  False         False  False  False  \n",
      "\n",
      "[712045 rows x 11 columns]\n",
      "False\n",
      "le nombre de jours non remplis est de  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Nb de jours\", df_full_train[\"Day\"].size)\n",
    "print(\"Nb de magasins\", df_full_train[\"ID\"].size)\n",
    "print(\"Nb de Customers\", df_full_train[\"Number_of_Customers\"].size)\n",
    "print(\"Nb de open\", df_full_train[\"Open\"].size)\n",
    "print(\"Nb de Promo\", df_full_train[\"Promo\"].size)\n",
    "print(\"Nb de State_Holiday\", df_full_train[\"State_Holiday\"].size)\n",
    "print(\"Nb de School_Holiday\", df_full_train[\"School_Holiday\"].size)\n",
    "print(\"Nb de Sales\", df_full_train[\"Sales\"].size)\n",
    "\n",
    "Day_nul = df_full_train.isna()[\"Day\"]\n",
    "print(df_full_train.isna())\n",
    "print(Day_nul[1])\n",
    "nb_jour_nn_remplis = 0\n",
    "for i in range(df_full_train[\"Day\"].size-1) :\n",
    "    if Day_nul[i] :\n",
    "        nb_jour_nn_remplis +1\n",
    "\n",
    "print(\"le nombre de jours non remplis est de \",nb_jour_nn_remplis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Découper les données de test et d'apprentissage pour voir l'intéret des différents algorithmes sur notre propre set d'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN pour savoir si ya des ventes ou pas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour celà il faut redéfinir les données de la solution en variable binaire : 0 si pas de vente et 1 si la vente>0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sales\n",
      "0   7293\n",
      "1   7060\n",
      "2   4565\n",
      "3   6380\n",
      "4  11647\n",
      "5   7967\n",
      "6   7101\n",
      "7      0\n",
      "8   7516\n",
      "9   6295\n",
      "   Sales\n",
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "5      1\n",
      "6      1\n",
      "7      0\n",
      "8      1\n",
      "9      1\n"
     ]
    }
   ],
   "source": [
    "print(df_sol[:10])\n",
    "df_sol2=pd.DataFrame(df_sol,dtype=\"bool\")\n",
    "df_sol2=pd.DataFrame(df_sol2,dtype=\"int\")\n",
    "print(df_sol2[:10])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,z_train,z_test=train_test_split(df_train,df_sol2,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnau\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1235197627135207e-05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "digit_knn=knn.fit(X_train, z_train) \n",
    "# Estimation de l'erreur de prévision\n",
    "# sur l'échantillon test\n",
    "1-digit_knn.score(X_test,z_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score obtenu est stylé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnau\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:715: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# grille de valeurs\n",
    "param=[{\"n_neighbors\":list(range(1,15))}]\n",
    "knn= GridSearchCV(KNeighborsClassifier(),param,cv=5,n_jobs=-1)\n",
    "digit_knnOpt=knn.fit(X_train, z_train)\n",
    "# paramètre optimal\n",
    "digit_knnOpt.best_params_[\"n_neighbors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.12351976e-05])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_test_tolist = z_test.values.tolist()\n",
    "\n",
    "z_pred = digit_knnOpt.predict(X_test)\n",
    "\n",
    "a = np.array(z_test_tolist) # your x\n",
    "b = np.array(z_pred) # your y\n",
    "mse = 0\n",
    "for i in range(len(a)):\n",
    "    mse+= (a[i]-b[i])**2/len(a)\n",
    "\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yeeeeaaah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,z_train,z_test=train_test_split(df_train,df_sol,test_size=0.2,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arnau\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\arnau\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\arnau\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression()\n",
    "Sales_logit=logit.fit(X_train, z_train)\n",
    "z_chap = Sales_logit.predict(X_test)\n",
    "# Erreur sur l'écahntillon test\n",
    "#1-titan_logit.score(T_test, z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE=\",mean_squared_error(z_test,z_chap))\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R2=\",r2_score(z_test,z_chap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbre de décision binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,z_train,z_test=train_test_split(df_train,df_sol,test_size=0.2,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Optimisation de la profondeur de l'arbre\n",
    "param=[{\"max_depth\":list(range(2,10))}]\n",
    "tree= GridSearchCV(DecisionTreeClassifier(),param,cv=10,n_jobs=-1)\n",
    "tree.fit(X_train, z_train)\n",
    "z_chap = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE=\",mean_squared_error(z_test,z_chap))\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R2=\",r2_score(z_test,z_chap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,z_train,z_test=train_test_split(df_train,df_sol,train_size=1000,test_size=1000,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# définition des paramètres\n",
    "#forest = RandomForestClassifier(n_estimators=10, \n",
    "#   criterion='gini', max_depth=None,\n",
    "#   min_samples_split=200, min_samples_leaf=1, \n",
    "#   max_features='auto', max_leaf_nodes=None,\n",
    "#   bootstrap=True, oob_score=True)\n",
    "forest2 = RandomForestClassifier(n_estimators=200, criterion='entropy')\n",
    "# apprentissage et erreur out-of-bag\n",
    "#forest = forest.fit(X_train,z_train)\n",
    "forest2 = forest2.fit(X_train,z_train)\n",
    "z_chap = forest2.predict(X_test)\n",
    "#print(1-forest.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE=\",mean_squared_error(z_test,z_chap))\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R2=\",r2_score(z_test,z_chap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest felix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from functools import reduce\n",
    "\n",
    "def generate_rf(X_train, y_train, X_test, y_test, n_estims):\n",
    "    rf = RandomForestClassifier(n_estimators=n_estims, \n",
    "               criterion='gini', max_depth=None,\n",
    "               min_samples_split=2, min_samples_leaf=1, \n",
    "               max_features='auto', max_leaf_nodes=None,\n",
    "               bootstrap=True)\n",
    "    #rf = RandomForestClassifier(n_estimators=10, min_samples_leaf=3)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print(\"*\")\n",
    "    return rf\n",
    "\n",
    "def combine_rfs(rf_a, rf_b):\n",
    "    rf_a.estimators_ += rf_b.estimators_\n",
    "    rf_a.n_estimators = len(rf_a.estimators_)\n",
    "    return rf_a\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train,df_sol,train_size=2000,test_size=1000,random_state=42)\n",
    "# in the line below, we create n_forests random forest classifier models\n",
    "n_estims = 3\n",
    "n_forests = 100\n",
    "rfs = [generate_rf(X_train, y_train, X_test, y_test,n_estims) for i in range(n_forests)]\n",
    "# in this step below, we combine the list of random forest models into one giant model\n",
    "rf_combined = reduce(combine_rfs, rfs)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation set de données pour le test uniquement\n",
    "#X_train,X_test,z_train,z_test= train_test_split(df_train,df_sol,test_size=10000,random_state=42)\n",
    "\n",
    "z_chap = rf_combined.predict(df_test)\n",
    "print(\"MSE=\",mean_squared_error(z_test,z_chap))\n",
    "print(\"R2=\",r2_score(z_test,z_chap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,z_train,z_test=train_test_split(df_train,df_sol,test_size=0.25,random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "tf = StandardScaler()\n",
    "tf.fit(X_train, z_train)\n",
    "Xt_train = tf.transform(X_train)  \n",
    "print(\"Moyenne avant centrage et réduction =\", np.mean(X_train))\n",
    "print(\"Moyenne après centrage et réduction =\", np.mean(Xt_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raccourci: Xt = tf.fit_transform(X)\n",
    "tf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB. La standardisation préalable est indispensable pour certains algorithmes\n",
    "# notamment les SVM\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "# Calcul des scores (bien classés)\n",
    "print(\"Sans standardisation =\", clf.fit(X_train, z_train).score(X_test, z_test))\n",
    "print(\"Avec standardisation =\", clf.fit(tf.transform(X_train), z_train).score(tf.transform(X_test), z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(), \n",
    "                    RFE(RandomForestClassifier(), n_features_to_select=10), \n",
    "                    RandomForestClassifier())\n",
    "clf.fit(X_train, z_train)\n",
    "z_chap = clf.predict(X_test)\n",
    "print(clf.predict_proba(X_test)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE=\",mean_squared_error(z_test,z_chap))\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R2=\",r2_score(z_test,z_chap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composition emboitée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données initiales sont unies aux composantes de l'ACP, puis les variables les plus importantes au sens des forêts aléatoires sont sélectionnées avant de servir à l'apprentissage d'un réseau de neurones. Ce n'est sûrement pas une stratégie optimale !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,z_train,z_test=train_test_split(df_train,df_sol,test_size=0.3,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import make_union\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "clf = make_pipeline(\n",
    "    # Build features\n",
    "    make_union(\n",
    "        FunctionTransformer(func=lambda X: X), PCA(),), \n",
    "    # Select the best features\n",
    "    RFE(RandomForestClassifier()),\n",
    "    # Train\n",
    "    MLPClassifier(max_iter=500)\n",
    ")\n",
    "\n",
    "clf.fit(X_train, z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_chap = clf.predict(X_test)\n",
    "\n",
    "print (z_chap.size)\n",
    "print(z_test.size)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE=\",mean_squared_error(z_test,z_chap))\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R2=\",r2_score(z_test,z_chap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression linéaire sur les composantes principales (repris du code d'Arnaud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,z_train,z_test=train_test_split(df_train,df_sol,test_size=200,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "# L'algorithme ds réseaux de neurones nécessite éventuellement une normalisation \n",
    "# des variables explicatives avec les commandes ci-dessous\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "Xr_train = scaler.transform(X_train)  \n",
    "# Meme transformation sur le test\n",
    "Xr_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# Erreur quadratique moyenne\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "regLasso = linear_model.Lasso()\n",
    "regLasso.fit(Xr_train,z_train)\n",
    "z_chap=regLasso.predict(Xr_test)\n",
    "print(\"MSE=\",mean_squared_error(z_test,z_chap))\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R2=\",r2_score(z_test,z_chap))\n",
    "print(prev.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseaux de neurones MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,z_train,z_test=train_test_split(df_train,df_sol,train_size = 5000, test_size=2000,random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "# L'algorithme ds réseaux de neurones nécessite éventuellement une normalisation \n",
    "# des variables explicatives avec les commandes ci-dessous\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "Xr_train = scaler.transform(X_train)  \n",
    "# Meme transformation sur le test\n",
    "Xr_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "myNN = MLPRegressor(hidden_layer_sizes=(200,), activation='logistic', solver='lbfgs', max_iter=2000, learning_rate=\"constant\")\n",
    "#myNN = MLPRegressor(hidden_layer_sizes=(300,), activation='logistic', solver='lbfgs',max_iter=1000, warm_start=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myNN.fit(Xr_train,z_train)\n",
    "#myNN.fit(X_train,z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_chap = myNN.predict(Xr_test)\n",
    "z_chap2 = myNN.predict(Xr_test)\n",
    "#z_chap = myNN.predict(X_test)\n",
    "#z_chap2 = myNN.predict(X_test)\n",
    "print(z_chap.size)\n",
    "print(type(z_chap))\n",
    "for i in range(z_chap.size) :\n",
    "    if z_chap[i]<0 :\n",
    "        z_chap2[i]=0\n",
    "print(z_chap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE=\",mean_squared_error(z_test,z_chap))\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R2=\",r2_score(z_test,z_chap))\n",
    "print(\"MSE2=\",mean_squared_error(z_test,z_chap2))\n",
    "print(\"R22=\",r2_score(z_test,z_chap2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
